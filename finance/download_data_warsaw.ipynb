{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22it [00:02,  5.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 690 ms, sys: 170 ms, total: 860 ms\n",
      "Wall time: 9.97 s\n"
     ]
    }
   ],
   "source": [
    "from datetime import timedelta, date, datetime\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from lxml.html import fromstring\n",
    "\n",
    "def date_range(start_date, end_date):\n",
    "    \"\"\"\n",
    "    find business days to parse\n",
    "    :param start_date:\n",
    "    :param end_date:\n",
    "    :return: business days \n",
    "    \"\"\"\n",
    "    for n in range(int((end_date - start_date).days) + 1):\n",
    "        if (start_date + timedelta(n)).weekday() not in (5, 6):\n",
    "            yield start_date + timedelta(n)\n",
    "\n",
    "def generate_links(dates, url):\n",
    "    \"\"\"generate links to parse from\"\"\"\n",
    "    \n",
    "    for date in dates:\n",
    "        link = url.format(date)\n",
    "        yield link\n",
    "\n",
    "async def fetch(session, url):\n",
    "    \"\"\"download data from url and return it in json\"\"\"\n",
    "    with aiohttp.Timeout(10):\n",
    "        async with session.get(url) as response:\n",
    "            html = await response.text()\n",
    "            tree = fromstring(html)\n",
    "            text = []\n",
    "            \n",
    "            for tr in tree.xpath('//tr'):\n",
    "                text.append([x.strip().replace(u'\\xa0', u'') for x in tr.text_content().split('\\n')])\n",
    "            \n",
    "            names = [\n",
    "                \"not_used1\",\n",
    "                \"Short\",\n",
    "                \"ISIN\",\n",
    "                \"Currency\",\n",
    "                \"Open\",\n",
    "                \"High\",\n",
    "                \"Low\",\n",
    "                \"Closing\",\n",
    "                \"% daily change *\",\n",
    "                \"Volume\",\n",
    "                \"Number of trades\",\n",
    "                \"Turnover (thous.)\",\n",
    "                \"not_used2\"]\n",
    "            \n",
    "            output = {}\n",
    "            output[url[-10:]] = [dict(zip(names, t)) for indx, t in enumerate(text) if indx != 0]\n",
    "\n",
    "            return json.dumps(output, indent=4, sort_keys=True)\n",
    "    \n",
    "async def fetch_all(loop, urls, sem):\n",
    "    \"\"\"task manager\"\"\"\n",
    "    async with aiohttp.ClientSession(loop=loop) as session:\n",
    "        tasks = []\n",
    "        for url in tqdm(urls):\n",
    "            await sem.acquire()\n",
    "            task = asyncio.ensure_future(fetch(session, url))\n",
    "            task.add_done_callback(lambda t: sem.release())\n",
    "            task.add_done_callback(save_to_file)\n",
    "            task.add_done_callback(tasks.remove)\n",
    "            tasks.append(task)\n",
    "        \n",
    "        results = await asyncio.gather(*tasks, return_exceptions=True)  # default is false, that would raise\n",
    "        \n",
    "    # for testing purposes only\n",
    "    # gather returns results in the order of coros, works if urls is a list\n",
    "    [print('ERROR: ', url) for url in results if isinstance(url, BaseException)]\n",
    "            \n",
    "    return results\n",
    "\n",
    "def save_to_file(data_save):\n",
    "    \"\"\"save data to filename\"\"\"\n",
    "    with open(\"{}.txt\".format('warsaw_futures1'), \"a\") as f:\n",
    "        for i in data_save.result():\n",
    "            f.write(i)\n",
    "    # print(\"Saved to file {}\".format(f.name))\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "\n",
    "    start = date(2016, 9, 17)\n",
    "    end = date(2016, 10, 18)\n",
    "    # index\n",
    "    # url = 'https://www.gpw.pl/notowania_archiwalne_full_en?type=1&date={}'\n",
    "    # futures\n",
    "    url = 'https://www.gpw.pl/notowania_archiwalne_full_en?type=35&date={}'\n",
    "    \n",
    "    headers = {'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "    'Accept-Encoding': 'gzip, deflate, sdch, br',\n",
    "    'Accept-Language': 'ru-RU,ru;q=0.8,en-US;q=0.6,en;q=0.4',\n",
    "    'Upgrade-Insecure-Requests': 1,\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36',\n",
    "    'Referer': 'https://www.gpw.pl/notowania_archiwalne_en?type=1&date=2016-10-12&show.x=32&show.y=12',\n",
    "    'Cookie': 'PHPSESSID=9ms5i5ot338lkmorctqp7uf0o4; SID=!Khgk9sVQQ1zJmCEx3T9eKbrcTnC4jFE0LGIk/49Mys+u4PdrgyyJb0IcJBmyj7v7VF/taLxvsP5lqyaqEOoMZ6TrVZn9eTFrkLPO6nsSigfv2CbqJEbbVNDh9ZvRKVhSC6KG8zuzU3TIMe/hWIBqmwKcELYW7uw=; lang_code=EN; TS014040d7=016c1ed7ff34b3ee8cd0041f3724d83f5f0b455c8c9163223d541f4b3fbd0dfaf0f8ef6af01d96c806f14c8fd0f025c4cc8135b8ca32fae5a197c21b401a054bd3992bbdedcd89131a55b62bb9536890c98fda94c7',\n",
    "    'Connection': 'keep-alive'}\n",
    "    \n",
    "    loop = asyncio.get_event_loop()\n",
    "    sem = asyncio.Semaphore(10, loop=loop)\n",
    "    urls = generate_links(date_range(start, end), url)\n",
    "    future = asyncio.ensure_future(fetch_all(loop, urls, sem))\n",
    "    res = %time loop.run_until_complete(future)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
