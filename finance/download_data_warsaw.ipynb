{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "81it [00:59,  5.75s/it]\n",
      "92it [01:00,  2.94s/it]\n",
      "93it [01:01,  2.22s/it]\n",
      "25it [00:02,  9.78it/s]\u001b[A\n",
      "94it [01:01,  1.72s/it]\n",
      "95it [01:02,  1.37s/it]\n",
      "96it [01:03,  1.12s/it]\n",
      "97it [01:03,  1.05it/s]\n",
      "31it [00:04,  2.66it/s]\u001b[A\n",
      "98it [01:04,  1.18it/s]\n",
      "99it [01:04,  1.32it/s]\n",
      "100it [01:05,  1.45it/s]\n",
      "101it [01:05,  1.52it/s]\n",
      "102it [01:06,  1.59it/s]\n",
      "37it [00:07,  2.15it/s]\u001b[A\n",
      "104it [01:07,  1.92it/s]\n",
      "39it [00:07,  2.22it/s]\u001b[A\n",
      "105it [01:07,  1.94it/s]\n",
      "106it [01:08,  1.90it/s]\n",
      "107it [01:08,  1.88it/s]\n",
      "108it [01:09,  1.88it/s]\n",
      "44it [00:10,  1.94it/s]\u001b[A\n",
      "115it [01:10,  2.36it/s]\n",
      "46it [00:11,  2.28it/s]\u001b[A\n",
      "49it [00:11,  2.97it/s]\u001b[A\n",
      "118it [01:11,  2.31it/s]\n",
      "119it [01:12,  2.32it/s]\n",
      "120it [01:12,  2.23it/s]\n",
      "121it [01:13,  2.16it/s]\n",
      "122it [01:13,  2.23it/s]\n",
      "124it [01:14,  2.19it/s]\n",
      "126it [01:15,  2.23it/s]\n",
      "127it [01:15,  2.29it/s]\n",
      "128it [01:16,  2.23it/s]\n",
      "129it [01:16,  2.17it/s]\n",
      "130it [01:17,  2.30it/s]\n",
      "132it [01:17,  2.61it/s]\n",
      "133it [01:18,  2.43it/s]\n",
      "134it [01:18,  2.33it/s]\n",
      "135it [01:19,  2.25it/s]\n",
      "139it [01:19,  3.50it/s]\n",
      "140it [01:20,  2.86it/s]\n",
      "141it [01:20,  2.64it/s]\n",
      "69it [00:21,  2.67it/s]\u001b[A\n",
      "142it [01:20,  2.68it/s]\n",
      "145it [01:22,  2.36it/s]\n",
      "146it [01:22,  2.31it/s]\n",
      "147it [01:23,  2.30it/s]\n",
      "148it [01:23,  2.29it/s]\n",
      "150it [01:24,  2.70it/s]\n",
      "151it [01:24,  2.48it/s]\n",
      "152it [01:24,  2.45it/s]\n",
      "153it [01:25,  2.35it/s]\n",
      "154it [01:25,  2.38it/s]\n",
      "82it [00:26,  2.81it/s]\u001b[A\n",
      "155it [01:26,  2.34it/s]\n",
      "157it [01:26,  2.79it/s]\n",
      "158it [01:27,  3.05it/s]\n",
      "161it [01:28,  2.87it/s]\n",
      "162it [01:28,  3.15it/s]\n",
      "163it [01:29,  2.64it/s]\n",
      "166it [01:29,  3.35it/s]\n",
      "90it [00:30,  1.99it/s]\u001b[A\n",
      "167it [01:30,  2.26it/s]\n",
      "169it [01:30,  2.95it/s]\n",
      "171it [01:31,  3.68it/s]\n",
      "178it [01:32,  4.69it/s]\n",
      "179it [01:32,  4.91it/s]\n",
      "182it [01:33,  4.38it/s]\n",
      "183it [01:33,  4.84it/s]\n",
      "186it [01:34,  4.31it/s]\n",
      "187it [01:34,  4.67it/s]\n",
      "191it [01:35,  4.79it/s]\n",
      "194it [01:35,  5.20it/s]\n",
      "105it [00:36,  2.19it/s]\u001b[A\n",
      "196it [01:37,  2.41it/s]\n",
      "200it [01:38,  3.08it/s]\n",
      "202it [01:39,  2.58it/s]\n",
      "110it [00:39,  1.47it/s]"
     ]
    }
   ],
   "source": [
    "from datetime import timedelta, date, datetime\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from lxml.html import fromstring\n",
    "\n",
    "def date_range(start_date, end_date):\n",
    "    \"\"\"\n",
    "    find business days to parse\n",
    "    :param start_date:\n",
    "    :param end_date:\n",
    "    :return: generator with business days\n",
    "    \"\"\"\n",
    "    for n in range(int((end_date - start_date).days) + 1):\n",
    "        if (start_date + timedelta(n)).weekday() not in (5, 6):\n",
    "            yield start_date + timedelta(n)\n",
    "\n",
    "def generate_links(dates, url):\n",
    "    \"\"\"\n",
    "    generate links to parse from\n",
    "    \"\"\"\n",
    "\n",
    "    for date in dates:\n",
    "        link = url.format(date)\n",
    "        yield link\n",
    "\n",
    "async def fetch(session, url):\n",
    "    with aiohttp.Timeout(10):\n",
    "        async with session.get(url) as response:\n",
    "            html = await response.text()\n",
    "            tree = fromstring(html)\n",
    "            text = []\n",
    "            \n",
    "            for tr in tree.xpath('//tr'):\n",
    "                text.append([x.strip().replace(u'\\xa0', u'') for x in tr.text_content().split('\\n')])\n",
    "            \n",
    "            names = [\n",
    "                \"not_used1\",\n",
    "                \"Short\",\n",
    "                \"ISIN\",\n",
    "                \"Currency\",\n",
    "                \"Open\",\n",
    "                \"High\",\n",
    "                \"Low\",\n",
    "                \"Closing\",\n",
    "                \"% daily change *\",\n",
    "                \"Volume\",\n",
    "                \"Number of trades\",\n",
    "                \"Turnover (thous.)\",\n",
    "                \"not_used2\"]\n",
    "            \n",
    "            output = {}\n",
    "            \n",
    "            \n",
    "            output[url[-10:]] = [dict(zip(names, t)) for indx, t in enumerate(text) if indx != 0]\n",
    "\n",
    "            return json.dumps(output, indent=4, sort_keys=True)\n",
    "\n",
    "def got_result(future):\n",
    "    print(future.result())\n",
    "    \n",
    "async def fetch_all(loop, urls, sem):\n",
    "    async with aiohttp.ClientSession(loop=loop) as session:\n",
    "        tasks = []\n",
    "        for url in tqdm(urls):\n",
    "            await sem.acquire()\n",
    "            task = asyncio.ensure_future(fetch(session, url))\n",
    "            task.add_done_callback(lambda t: sem.release())\n",
    "\n",
    "            task.add_done_callback(save_to_file)\n",
    "            task.add_done_callback(tasks.remove)\n",
    "            \n",
    "            tasks.append(task)\n",
    "        \n",
    "        results = await asyncio.gather(*tasks, return_exceptions=True)  # default is false, that would raise\n",
    "        #save_to_file(results)\n",
    "        \n",
    "    # for testing purposes only\n",
    "    # gather returns results in the order of coros\n",
    "    # works if urls is a list\n",
    "    for url in results:\n",
    "        if isinstance(url, BaseException):\n",
    "            print('ERROR: ', url)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def save_to_file(data_save):\n",
    "    \"\"\"save data to filename\"\"\"\n",
    "    with open(\"{}.txt\".format('warsaw_index'), \"a\") as f:\n",
    "        for i in data_save.result():\n",
    "            f.write(i)\n",
    "    # print(\"Saved to file {}\".format(f.name))\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "\n",
    "    start = date(1994, 4, 18)\n",
    "    end = date(2016, 10, 18)\n",
    "    # index\n",
    "    url = 'https://www.gpw.pl/notowania_archiwalne_full_en?type=1&date={}'\n",
    "    # futures\n",
    "    # url = 'https://www.gpw.pl/notowania_archiwalne_full_en?type=35&date={}'\n",
    "    \n",
    "    headers = {'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "    'Accept-Encoding': 'gzip, deflate, sdch, br',\n",
    "    'Accept-Language': 'ru-RU,ru;q=0.8,en-US;q=0.6,en;q=0.4',\n",
    "    'Upgrade-Insecure-Requests': 1,\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36',\n",
    "    'Referer': 'https://www.gpw.pl/notowania_archiwalne_en?type=1&date=2016-10-12&show.x=32&show.y=12',\n",
    "    'Cookie': 'PHPSESSID=9ms5i5ot338lkmorctqp7uf0o4; SID=!Khgk9sVQQ1zJmCEx3T9eKbrcTnC4jFE0LGIk/49Mys+u4PdrgyyJb0IcJBmyj7v7VF/taLxvsP5lqyaqEOoMZ6TrVZn9eTFrkLPO6nsSigfv2CbqJEbbVNDh9ZvRKVhSC6KG8zuzU3TIMe/hWIBqmwKcELYW7uw=; lang_code=EN; TS014040d7=016c1ed7ff34b3ee8cd0041f3724d83f5f0b455c8c9163223d541f4b3fbd0dfaf0f8ef6af01d96c806f14c8fd0f025c4cc8135b8ca32fae5a197c21b401a054bd3992bbdedcd89131a55b62bb9536890c98fda94c7',\n",
    "    'Connection': 'keep-alive'}\n",
    "    \n",
    "    loop = asyncio.get_event_loop()\n",
    "    sem = asyncio.Semaphore(10, loop=loop)\n",
    "\n",
    "    urls = generate_links(date_range(start, end), url)\n",
    "    future = asyncio.ensure_future(fetch_all(loop, urls, sem))\n",
    "    res = %time loop.run_until_complete(future)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['{\\n    \"1994-04-20\": []\\n}', '{\\n    \"1994-04-22\": []\\n}']\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
